name: Fetch updated data and deploy

on:
  push:
  schedule:
    # - cron: '5,35 * * * *'
    - cron: '45 16 * * *'

jobs:
  build_and_deploy:
    runs-on: ubuntu-latest
    steps:
    - name: Check out repo
      uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v1
      with:
        python-version: 3.8
    - uses: actions/cache@v2
      name: Configure pip caching
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    - uses: actions/cache@v2
      name: Cache dbs folder
      with:
        path: dbs
        key: ${{ runner.os }}-dbs-
        restore-keys: |
          ${{ runner.os }}-dbs-
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    - name: Ensure dbs/ exists
      run: mkdir -p dbs
    - name: Delete content of dbs/ if REFRESH_DBS
      if: |-
        contains(github.event.head_commit.message, 'REFRESH_DBS')
      run: |-
        rm dbs/*
    - name: Download existing .db files
      if: |-
        !contains(github.event.head_commit.message, 'REFRESH_DBS') && !(github.event_name == 'schedule' && github.event.schedule == '20 17 * * *')
      env:
        DATASETTE_TOKEN: ${{ secrets.DATASETTE_TOKEN }}
      run: |-
        datasette-clone https://biglocal.datasettes.com/ dbs -v --token=$DATASETTE_TOKEN
    - name: Fetch projects
      env:
        BIGLOCAL_TOKEN: ${{ secrets.BIGLOCAL_TOKEN }}
      run: |-
        python fetch_projects.py dbs/biglocal.db $BIGLOCAL_TOKEN \
          --contact biglocalnews@stanford.edu \
          --skip='UHJvamVjdDowM2ZiMjA5NS03MzcxLTRjODEtOTMzNi05YTFiZGY2YWE2NDU=' \
          --skip='UHJvamVjdDo4NTBjOWJmYy03YzAyLTRkNDgtYjYzMS04OThhODFmZjQxNDQ='
        # Skipped COVID_Johns_Hopkins_daily_reports and COVID_twitter_data
    - name: Populate tables
      env:
        BIGLOCAL_TOKEN: ${{ secrets.BIGLOCAL_TOKEN }}
      run: |-
        cd dbs && python ../populate_tables.py biglocal.db $BIGLOCAL_TOKEN
    - name: Update metadata.json with project descriptions
      run: |-
        python update_metadata.py dbs/biglocal.db metadata.json production-metadata.json
    - name: List files in dbs
      run: |-
        ls -lah dbs
    - name: Set up Cloud Run
      uses: GoogleCloudPlatform/github-actions/setup-gcloud@master
      if: github.ref == 'refs/heads/master'
      with:
        version: '275.0.0'
        service_account_email: ${{ secrets.GCP_SA_EMAIL }}
        service_account_key: ${{ secrets.GCP_SA_KEY }}
    - name: Deploy to Cloud Run
      if: github.ref == 'refs/heads/master'
      env:
        DATASETTE_TOKEN: ${{ secrets.DATASETTE_TOKEN }}
        GH_CLIENT_ID: ${{ secrets.GH_CLIENT_ID }}
        GH_CLIENT_SECRET: ${{ secrets.GH_CLIENT_SECRET }}
        LATEST_DB_HASH: ${{ steps.decide_variables.outputs.latest_db_hash }}
      run: |-
        gcloud config set run/region us-central1
        gcloud config set project datasette-222320
        datasette publish cloudrun dbs/*.db \
           --service=biglocal \
           --install=datasette-auth-github \
           --install=datasette-render-markdown \
           --install=datasette-vega \
           --install=datasette-dateutil \
           --install=datasette-graphql \
           --install=datasette-copyable \
           --install=datasette-dateutil \
           --install=datasette-rure \
           --plugin-secret token-auth secret $DATASETTE_TOKEN \
           --memory=3Gi \
           --static css:css \
           --plugins-dir=plugins \
           --metadata=production-metadata.json \
           --version-note=$LATEST_DB_HASH
